#!/usr/bin/env python
"""Upload the information on the number of reads per barcode after demultiplex generated by the analysis pipeline to a spreadsheet on Google docs.

Given the name of a sequencing run (including date, machine id and flowcell id), a run_info.yaml configuration file, the title of a (pre-existing) 
spreadsheet on a GoogleDocs account and a base64-encoded string of the concatenated username and password (separated by a ':') for a google account 
with write access to the spreadsheeta, will create a new worksheet in the spreadsheet and enter the data into it. Optionally, the name of the 
worksheet where data will be written can be specified, by default a new worksheet on the form [date]_[flowcell_id] is created. Alternatively, one 
worksheet can be created for each project included in the run.

"""
import os
import sys
import base64
import yaml
from optparse import OptionParser
from bcbio.pipeline import log
from bcbio.templates.bc_metrics import get_bc_stats

import gdata.spreadsheet.service
import gdata.docs.service

# The structure of the demultiplex result file on the form {title: column index}
header = [
          ['Project name','project_name'],
          ['Date','date'],
          ['Flowcell','flowcell_id'],
          ['Lane','lane'],
          ['Description','description'],
          ['Internal barcode index','barcode_id'],
          ['Barcode name','name'],
          ['Barcode sequence','sequence'],
          ['Barcode type','barcode_type'],
          ['Demultiplexed read count','barcode_read_count'],
          ['Comment','comment']
        ]

def decode_credentials(credentials):
    
    if not credentials:
        return None
    
    # Split the username and password
    return base64.b64decode(credentials).split(':',1);
    

def main(run_name, run_info_yaml, gdocs_spreadsheet, gdocs_credentials, analysis_dir, gdocs_worksheet, append, split_on_project):

    log.info("Processing run: %s" % run_name)
    
    # Split the username and password
    credentials = decode_credentials(gdocs_credentials)
    if not credentials:
        log.warn("Could not decode GDocs credentials. No demultiplex counts were written to Google Docs")
        return
    
    # Get the GDocs demultiplex result file title
    if not gdocs_spreadsheet:
        log.warn("Could not find Google Docs demultiplex results file title in config. No demultiplex counts were written to Google Docs")
        return
    
    # Get the barcode statistics
    if not os.path.exists(run_info_yaml):
        log.warn("Could not find required run_info.yaml configuration file at '%s'" % run_info_yaml)
        return
    with open(run_info_yaml) as in_handle:
        run_info = yaml.load(in_handle)

    fc_dir = os.path.join(analysis_dir,run_name)
    bc_metrics = get_bc_stats(run_info,fc_dir)
    
    upload_demultiplex_data(bc_metrics,gdocs_spreadsheet,credentials,gdocs_worksheet,append,split_on_project)
    
    
def upload_demultiplex_data(bc_metrics, gdocs_spreadsheet, credentials, gdocs_worksheet=None, append=False, split_on_project=False):
    
    # Convert the spreadsheet title to unicode
    gdocs_spreadsheet = _to_unicode(gdocs_spreadsheet)
       
    # Create a client class which will make HTTP requests with Google Docs server.
    client = gdata.spreadsheet.service.SpreadsheetsService()
    client.email = credentials[0]
    client.password = credentials[1]
    client.source = 'demultiplex_upload_to_gdocs.py'
    client.ProgrammaticLogin()
    
    # Create a query that restricts the spreadsheet feed to documents having the supplied title
    q = gdata.spreadsheet.service.DocumentQuery(params={'title':_from_unicode(gdocs_spreadsheet), 'title-exact':'true'})
    # Query the server for an Atom feed containing a list of your documents.
    feed = client.GetSpreadsheetsFeed(query=q)
    
    # Check that we got a result back
    if len(feed.entry) == 0:
        log.info("No document with specified title '%s' found in GoogleDocs repository" % gdocs_spreadsheet)
        return
    
    # If we get more than one feed item back, will have to implement some way of resolving these
    if len(feed.entry) > 1:
        log.info("More than one document match the specified title '%s', will have to implement some way of resolving these!" % gdocs_spreadsheet)
        return
    
    # Get the matching spreadsheet entry from the feed    
    spreadsheet = feed.entry[0]
    ss_key = spreadsheet.id.text.split('/')[-1]
    log.info("Found spreadsheet matching the supplied title: '%s', spreadsheet key is '%s'" % (spreadsheet.title.text,ss_key))
    
    # Convert the bc_metrics data structure into a flat list
    rows = _structure_to_list(bc_metrics)
    
    # Get the projects in the run
    projects = _get_project_names(rows)
    
    log.info("The run contains data from projects: %s" % ", ".join(projects))
    
    # If we will split the worksheet by project, use the project names as worksheet titles
    if split_on_project:
        # Filter away the irrelevent project entries and write the remaining to the appropriate worksheet
        for gdocs_worksheet in projects:
            _write_to_worksheet(client,ss_key,gdocs_worksheet,header,_apply_filter(rows,[gdocs_worksheet]),append)
            
    # Else, set the default title of the worksheet to be a string of concatenated date and flowcell id
    else:
        if gdocs_worksheet is None:
            gdocs_worksheet = "%s_%s" % (rows[0][1],rows[0][2])
        _write_to_worksheet(client,ss_key,gdocs_worksheet,header,rows,append)
        
    
def _write_to_worksheet(client,ss_key,ws_title,header,rows,append=False):
    
    # Convert the worksheet title to unicode
    ws_title = _to_unicode(ws_title)
    
    # Check if there already is a worksheet present matching the data
    ws = None
    q = gdata.spreadsheet.service.DocumentQuery(params={'title':_from_unicode(ws_title)})
    feed = client.GetWorksheetsFeed(key=ss_key,query=q)
    # If there already exists one or more matching worksheet(s)
    if len(feed.entry) > 0:
        # If we're appending, do it to the last entry in the feed
        if append:
            ws = feed.entry[-1]
        # Else, add an enumerating suffix to the ws title
        else:
            ws_title += "(%s)" % (len(feed.entry) + 1)
    
    # If necessary, create a new worksheet for this run and add it to the end of the spreadsheet
    if ws is None:
        log.info("Adding a new worksheet, '%s', to the spreadsheet" % ws_title)
        ws = client.AddWorksheet(ws_title,len(rows)+1,len(header),ss_key)
        if ws is None:
            log.info("Could not add a worksheet '%s' to spreadsheet with key '%s'" % (ws_title,ss_key))
            return
    
    ws_id = ws.id.text.split('/')[-1]

    log.info("Adding data to the '%s' worksheet, worksheet key is '%s'" % (ws_title,ws_id))
    # First, print the header
    # As a workaround for the InsertRow bugs with column names, just use single lowercase letters as column headers to start with
    for i in range(0,len(header)):
        client.UpdateCell(1,i+1,chr(97+i),ss_key,ws_id)
          
    # Iterate over the barcode stats and add the data to the worksheet
    for row in rows:
        row_data = {}
        for i,value in enumerate(row):
            row_data[chr(97+i)] = unicode(value)
        client.InsertRow(row_data,ss_key,ws_id)

    # Lastly, substitute the one-letter header for the real deal
    for i in range(0,len(header)):
        client.UpdateCell(1,i+1,header[i][0],ss_key,ws_id)

def _structure_to_list(structure):
    """Flatten all entries in the metrics data structure into a list of entry rows"""
    
    metrics_list = []
    for lane in structure:
        row = [""]*len(header)
        for i in range(0,5):
            row[i] = lane.get(header[i][1],"")

        for m in lane.get("multiplex",[]):
            for i in range(5,len(header)):
                row[i] = m.get(header[i][1],"")
                
            metrics_list.append(list(row))
            
    return metrics_list

def _get_project_names(rows):
    names = {}
    for row in rows:
        names[row[0]] = 1
    return names.keys()

def _apply_filter(unfiltered,filter):
    
    filtered = []
    for entry in unfiltered:
        passed = True
        for i,f in enumerate(filter):
            if f and entry[i] != f:
                passed = False
                break
        if passed:
            filtered.append(entry)
    
    return filtered

def _to_unicode(str,encoding='utf-8'):
    if isinstance(str,basestring):
        if not isinstance(str,unicode):
            str = unicode(str,encoding)
    return str

def _from_unicode(unistr,encoding='utf-8'):
    if isinstance(unistr,unicode):
        unistr = unistr.encode(encoding)
    return unistr

if __name__ == "__main__":
    usage = """
    demultiplex_upload_to_gdocs.py <run name> <run_info.yaml> <gdocs_spreadsheet> <gdocs_credentials>
                           [--analysis_dir=<analysis directory>
                            --gdocs_worksheet=<worksheet title>
                            --append
                            --split_on_project]
"""

    parser = OptionParser(usage=usage)
    parser.add_option("-b", "--analysis_dir", dest="analysis_dir", default=os.getcwd())
    parser.add_option("-w", "--gdocs_worksheet", dest="gdocs_worksheet", default=None)
    parser.add_option("-a", "--append", action="store_true", dest="append", default=False)
    parser.add_option("-s", "--split_on_project", action="store_true", dest="split_on_project", default=False)
    (options, args) = parser.parse_args()
    if len(args) < 1:
        print __doc__
        sys.exit()
    kwargs = dict(
        analysis_dir = os.path.normpath(options.analysis_dir),
        gdocs_worksheet = options.gdocs_worksheet,
        append = options.append,
        split_on_project = options.split_on_project
        )
    main(*args, **kwargs)
